{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b5a217-be0d-4e6c-aee4-6f7ba74c8178",
   "metadata": {},
   "source": [
    "# Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995a647",
   "metadata": {},
   "source": [
    "## 1-3: Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e80fd",
   "metadata": {},
   "source": [
    "Artificial Neurons를 구현할때,\n",
    "\n",
    "Affine Function뒤에 바로 Activation Function이 따라오는 게 가장 이상적이지만,\n",
    "\n",
    "딥러닝이 심화되면 이 두개 사이에 또다른 연산들이 들어갈 수 있어서 이런 상황을 대비하기 위해\n",
    "\n",
    "Activation Function만 만드는 것과 Affine Function과 같이 만드는 것 두가지 모두 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b32467",
   "metadata": {},
   "source": [
    "### Code.1-3-1: Activation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a370f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1, 5)\n",
      "[[ 0.43158635 -0.06011844  1.0767578   0.05733462 -0.16877407]]\n",
      "\n",
      "Sigmoid(Tensorflow): (1, 5)\n",
      "[[0.60625243 0.48497495 0.74587995 0.51432973 0.45790637]]\n",
      "Sigmoid(menual): (1, 5)\n",
      "[[0.60625243 0.48497495 0.74587995 0.51432973 0.45790637]]\n",
      "\n",
      "tand(Tensorflow): (1, 5)\n",
      "[[ 0.4066462  -0.06004617  0.7919937   0.05727187 -0.16718969]]\n",
      "tand(menual): (1, 5)\n",
      "[[ 0.40664616 -0.06004611  0.79199374  0.05727186 -0.16718967]]\n",
      "\n",
      "relu(Tensorflow): (1, 5)\n",
      "[[0.43158635 0.         1.0767578  0.05733462 0.        ]]\n",
      "relu(menual): (1, 5)\n",
      "[[0.43158635 0.         1.0767578  0.05733462 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp, maximum\n",
    "from tensorflow.keras.layers import Activation #activation 식을 layer로 가져왔다.\n",
    "\n",
    "x = tf.random.normal(shape=(1,5)) #input setting\n",
    "\n",
    "#imp.activation function\n",
    "sigmoid = Activation(\"sigmoid\")\n",
    "tanh = Activation(\"tanh\")\n",
    "relu = Activation(\"relu\")\n",
    "\n",
    "#forward propagation(tensorflow)\n",
    "y_sigmoid_tf = sigmoid(x)\n",
    "y_tanh_tf = tanh(x)\n",
    "y_relu_tf = relu(x)\n",
    "\n",
    "#forward propagation(manual)\n",
    "y_sigmoid_man = 1 /(1 + exp(-x))\n",
    "y_tanh_man = (exp(x) - exp(-x)) / (exp(x)+exp(-x))\n",
    "y_relu_man = maximum(x,0)\n",
    "\n",
    "print(\"x: {}\\n{}\\n\".format(x.shape, x.numpy()))\n",
    "print(\"Sigmoid(Tensorflow): {}\\n{}\".format(y_sigmoid_tf.shape, y_sigmoid_tf.numpy()))\n",
    "print(\"Sigmoid(menual): {}\\n{}\\n\".format(y_sigmoid_man.shape, y_sigmoid_man.numpy()))\n",
    "\n",
    "print(\"tand(Tensorflow): {}\\n{}\".format(y_tanh_tf.shape, y_tanh_tf.numpy()))\n",
    "print(\"tand(menual): {}\\n{}\\n\".format(y_tanh_man.shape, y_tanh_man.numpy()))\n",
    "\n",
    "print(\"relu(Tensorflow): {}\\n{}\".format(y_relu_tf.shape, y_relu_tf.numpy()))\n",
    "print(\"relu(menual): {}\\n{}\".format(y_relu_man.shape, y_relu_man.numpy()))\n",
    "\n",
    "#아래 결과를 보면 tensorflow로 구현한 것과 직접 activation function들의 식으로 구현한 것과 동일한 값이 나오는 것을 알 수 있다.\n",
    "#하나를 구현해보더라도 직접구현하는 습관이 중요.!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de45bb2",
   "metadata": {},
   "source": [
    "### Code.1-3-2: Activation in Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f766e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AN with Sigmoid: (1, 1)\n",
      "[[0.7773026]]\n",
      "AN with tanh: (1, 1)\n",
      "[[0.8482881]]\n",
      "AN with relu: (1, 1)\n",
      "[[1.250016]]\n",
      "\n",
      "==============\n",
      "\n",
      "Activation value(Tensorflow): (1, 1)\n",
      "[[0.7773026]]\n",
      "Activation value(menual): (1, 1)\n",
      "[[0.7773026]]\n"
     ]
    }
   ],
   "source": [
    "#아래 코드는 Affine Function과 Activation Function이 동시에 Dense로 한번에 진행되는 코드다.\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "x = tf.random.normal(shape = (1,5)) #input setting\n",
    "\n",
    "#imp. artificial neurons\n",
    "dense_sigmoid = Dense(units=1,activation=\"sigmoid\")\n",
    "dense_tanh = Dense(units=1,activation=\"tanh\")\n",
    "dense_relu = Dense(units=1,activation=\"relu\")\n",
    "\n",
    "#forward propagation\n",
    "y_sigmoid = dense_sigmoid(x)\n",
    "y_tanh = dense_tanh(x)\n",
    "y_relu = dense_relu(x)\n",
    "\n",
    "print(\"AN with Sigmoid: {}\\n{}\".format(y_sigmoid.shape,y_sigmoid.numpy()))\n",
    "print(\"AN with tanh: {}\\n{}\".format(y_tanh.shape,y_tanh.numpy()))\n",
    "print(\"AN with relu: {}\\n{}\".format(y_relu.shape,y_relu.numpy()))\n",
    "\n",
    "#forward propagation(manual)\n",
    "print(\"\\n==============\\n\")\n",
    "\n",
    "W, b = dense_sigmoid.get_weights()\n",
    "z = tf.linalg.matmul(x,W) + b #z 는 affine Function을 계산한 값이다.\n",
    "a = 1/ (1+exp(-z))\n",
    "print(\"Activation value(Tensorflow): {}\\n{}\".format(y_sigmoid.shape, y_sigmoid.numpy()))\n",
    "print(\"Activation value(menual): {}\\n{}\".format(a.shape, a.numpy()))\n",
    "# 위 두 코드의 결과값이 같다는 의미는 Dense(units=1,activation=\"sigmoid\")가 affine을 계산한 후에 바로 activation을 연산하였다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914d9df",
   "metadata": {},
   "source": [
    "## 1-4: Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d51ef6",
   "metadata": {},
   "source": [
    "### Code.1-4-1: Artificial Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b227b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which do you want activation Function?relu\n",
      "W:(10, 1)\n",
      "[[ 0.11144274]\n",
      " [ 0.6026682 ]\n",
      " [ 0.08686566]\n",
      " [-0.46665496]\n",
      " [-0.08183372]\n",
      " [ 0.68314713]\n",
      " [-0.40873873]\n",
      " [-0.2919103 ]\n",
      " [-0.23367286]\n",
      " [ 0.6926189 ]]\n",
      "\n",
      "b:(1,)\n",
      "[0.]\n",
      "\n",
      "\n",
      "Activation: relu\n",
      "y_tf: (1, 1)\n",
      "[[1.606822]]\n",
      "\n",
      "y_man: (1, 1)\n",
      "[[1.606822]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.math import exp, maximum\n",
    "\n",
    "activation = input(\"which do you want activation Function?\")\n",
    "#activation =\"sigmoid\"\n",
    "#activation =\"tanh\"\n",
    "#activation = 'relu'\n",
    "\n",
    "x = tf.random.uniform(shape=(1,10)) #input setting\n",
    "\n",
    "dense = Dense(units=1,activation = activation) #imp.. an affine + activation\n",
    "\n",
    "y_tf = dense(x)\n",
    "W,b = dense.get_weights()\n",
    "print(\"W:{}\\n{}\\n\".format(W.shape,W))\n",
    "print(\"b:{}\\n{}\\n\".format(b.shape,b))\n",
    "\n",
    "#calculate activation value manually\n",
    "y_man = tf.linalg.matmul(x,W)+b\n",
    "if activation ==\"sigmoid\":\n",
    "    y_man_activation = 1/(1+exp(-y_man))\n",
    "elif activation == \"tanh\":\n",
    "    y_man_activation = (exp(y_man)-exp(-y_man))/(exp(y_man)+exp(-y_man))\n",
    "elif activation == \"relu\":\n",
    "    y_man_activation = maximum(y_man,0)\n",
    "    \n",
    "print(\"\\nActivation:\",activation)\n",
    "print(\"y_tf: {}\\n{}\\n\".format(y_tf.shape,y_tf.numpy()))\n",
    "print(\"y_man: {}\\n{}\\n\".format(y_man_activation.shape,y_man_activation.numpy()))\n",
    "\n",
    "#꼭 이런 모델들이 어떻게 작동되는지 확인해보고 검증해보는 과정을 밟는 습관을 들이자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af568f2a",
   "metadata": {},
   "source": [
    "## 1-5: Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791aa1f",
   "metadata": {},
   "source": [
    "### Code.1-5-1: Shapes of Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd262b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:  (32000, 10)\n",
      "Shape of W:  (10, 1)\n",
      "Shape of b:  (1,)\n",
      "Shape of y:  (32000, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 32000,10 #set input params\n",
    "x = tf.random.normal(shape=(N,n_feature)) #minibatch 생성 x는 matrix형태로 이론에서 공부했던, (x)T형태의 모형이다.\n",
    "\n",
    "dense = Dense(units = 1, activation = \"relu\") #imp. an AN\n",
    "y = dense(x) #forward propagation\n",
    "\n",
    "W, b = dense.get_weights() #get weight/bias\n",
    "\n",
    "#print resluts\n",
    "print(\"Shape of x: \",x.shape)\n",
    "print(\"Shape of W: \",W.shape)\n",
    "print(\"Shape of b: \",b.shape)\n",
    "print(\"Shape of y: \",y.shape)\n",
    "\n",
    "#입력되는 matrix x의 개수와 상관없이 W와 b값은 변함이 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cadfd",
   "metadata": {},
   "source": [
    "### Code.1-5-2: Output Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f28b7607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output(tensorflow): \n",
      " [[0.75885034]\n",
      " [0.6597462 ]\n",
      " [0.619539  ]\n",
      " [0.46651772]\n",
      " [0.8563878 ]\n",
      " [0.35026667]\n",
      " [0.6869893 ]\n",
      " [0.5353104 ]]\n",
      "Output(manual): \n",
      " [[0.75885034]\n",
      " [0.6597462 ]\n",
      " [0.619539  ]\n",
      " [0.46651772]\n",
      " [0.8563878 ]\n",
      " [0.35026667]\n",
      " [0.6869893 ]\n",
      " [0.5353104 ]]\n"
     ]
    }
   ],
   "source": [
    "#위 코드에 이어서 activation Function까지 연산하는 거까지 한거임.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 8,10 #set input params\n",
    "x = tf.random.normal(shape=(N,n_feature)) #minibatch 생성 x는 matrix형태로 이론에서 공부했던, (x)T형태의 모형이다.\n",
    "\n",
    "dense = Dense(units = 1, activation = \"sigmoid\") #imp. an AN\n",
    "y_tf = dense(x) #forward propagation(tensorflow)\n",
    "\n",
    "W,b = dense.get_weights() #get weight/bias\n",
    "\n",
    "y_man = tf.linalg.matmul(x,W)+b #forward propagation(Manual)\n",
    "y_man_activation = 1/(1+tf.math.exp(-y_man))\n",
    "\n",
    "#print results\n",
    "print(\"Output(tensorflow): \\n\",y_tf.numpy())\n",
    "print(\"Output(manual): \\n\",y_man_activation.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635c19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
