{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a23e6d",
   "metadata": {},
   "source": [
    "# 2-1 Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0bed6-99b1-48dd-b733-9df8b6ea357e",
   "metadata": {},
   "source": [
    "코딩 과정\n",
    "dense layers 진행과정과 진행하면서 변화되는 입력데이터의 shape와 weight, bias의 shape도 확인해보자.\n",
    "1. x입력데이터는 tf.random.normal로 shape를 설정하여 만들자.\n",
    "2. dense 패키지를 활용하여 affine과 activation을 동시에 연산을 진행하는 객체를 만들고 거기에 x입력값을 넣자.\n",
    "3. 연산 다 된 상태에서 이제 W, B와 마지막으로 나온 출력값의 shape들을 확인해보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcb390",
   "metadata": {},
   "source": [
    "## Code.2-1-1:Shapes of Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6a9dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Input/Weight/Bias ====\n",
      "X:  (4, 10)\n",
      "W:  (10, 3)\n",
      "B:  (3,)\n",
      "Y:  (4, 3)\n",
      "tf.Tensor(\n",
      "[[0.30593953 0.2792285  0.73360217]\n",
      " [0.588831   0.22446041 0.6682987 ]\n",
      " [0.34605026 0.81149256 0.7125092 ]\n",
      " [0.5924157  0.50329536 0.6592485 ]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 4,10 #N은 데이터의 행개수, n_feature은 특성 즉 열의 개수\n",
    "X = tf.random.normal(shape=(N,n_feature))\n",
    "\n",
    "n_neuron = 3\n",
    "dense = Dense(units = n_neuron, activation = \"sigmoid\")\n",
    "Y_tf = dense(X)\n",
    "\n",
    "W, B = dense.get_weights()\n",
    "\n",
    "print('==== Input/Weight/Bias ====')\n",
    "print('X: ', X.shape)\n",
    "print('W: ', W.shape)\n",
    "print('B: ', B.shape)\n",
    "print('Y: ', Y_tf.shape)\n",
    "print(Y_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4fa8f6",
   "metadata": {},
   "source": [
    "## Code.2-1-2: Output Calculations (실제 연산으로 진행해보자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad7854e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(Tensorflow):\n",
      " [[0.43203837 0.05802592 0.50114614]\n",
      " [0.47051385 0.04984962 0.2801825 ]\n",
      " [0.17265202 0.752611   0.45432004]\n",
      " [0.7772887  0.54309726 0.30602607]]\n",
      "tf.Tensor(\n",
      "[[0.43203837 0.05802592 0.50114614]\n",
      " [0.47051385 0.04984962 0.2801825 ]\n",
      " [0.17265202 0.752611   0.45432004]\n",
      " [0.7772887  0.54309726 0.30602607]], shape=(4, 3), dtype=float32)\n",
      "Y(with multiplication): \n",
      " [[0.43203837 0.05802592 0.50114614]\n",
      " [0.47051385 0.04984962 0.2801825 ]\n",
      " [0.17265202 0.752611   0.45432004]\n",
      " [0.7772887  0.54309726 0.30602607]]\n",
      "Y(with dot products): \n",
      " [[0.43203831 0.05802592 0.50114614]\n",
      " [0.47051391 0.04984962 0.28018251]\n",
      " [0.17265199 0.75261092 0.4543201 ]\n",
      " [0.77728868 0.54309726 0.30602607]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.linalg import matmul\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_feature = 4,10 #N은 데이터의 행개수, n_feature은 특성 즉 열의 개수\n",
    "X = tf.random.normal(shape=(N,n_feature))\n",
    "\n",
    "n_neuron = 3\n",
    "dense = Dense(units = n_neuron, activation = \"sigmoid\")\n",
    "Y_tf = dense(X)\n",
    "\n",
    "W, B = dense.get_weights()\n",
    "print(\"Y(Tensorflow):\\n\",Y_tf.numpy())\n",
    "\n",
    "#calculate with matrix multiplication - 메트릭스 형태로 바로 곱을 하는 것.\n",
    "Z = matmul(X, W)+B\n",
    "Y_man_matmul= 1/(1+exp(-Z))\n",
    "print(\"Y(with multiplication): \\n\", Y_man_matmul.numpy())\n",
    "\n",
    "#calculate with dot products - dot product형태로 곱을하여 Yo_man_vec에 모두 모은 것.\n",
    "Y_man_vec = np.zeros(shape=(N, n_neuron))\n",
    "#print(Y_man_vec,\"가 아래처럼변함\")\n",
    "for x_idx in range(N):\n",
    "    x = X[x_idx]\n",
    "    \n",
    "    for nu_idx in range(n_neuron):\n",
    "        w, b = W[:,nu_idx],B[nu_idx]\n",
    "        z = tf.reduce_sum(x *w)+b\n",
    "        \n",
    "        a = 1/(1+exp(-z))\n",
    "        Y_man_vec[x_idx,nu_idx] = a #Y_man_vec metrix에 a값을 입력시켜준다. 행위치는 x_idx, 열위치는 nu_idx다.\n",
    "        \n",
    "print(\"Y(with dot products): \\n\", Y_man_vec)\n",
    "\n",
    "#하나하나의 과정들을 볼수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb17b5",
   "metadata": {},
   "source": [
    "# 2-2: Cascaded Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71044552",
   "metadata": {},
   "source": [
    "## Code.2-2-1: Shapes of Cacaded Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ec0378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : (4, 10)\n",
      "\n",
      "W1 : (10, 3)\n",
      "B1 : (3,)\n",
      "A1 : (4, 3)\n",
      "\n",
      "W2 : (3, 5)\n",
      "B2 : (5,)\n",
      "Y : (4, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "N, n_features =4, 10\n",
    "X = tf.random.normal(shape=(N,n_feature))\n",
    "\n",
    "n_neurons = [3,5]\n",
    "dense1 = Dense(units = n_neurons[0], activation = 'sigmoid')\n",
    "dense2 = Dense(units = n_neurons[1], activation = 'sigmoid')\n",
    "\n",
    "# forward propagation\n",
    "A1 = dense1(X)\n",
    "Y = dense2(A1)\n",
    "\n",
    "#get weight/bias\n",
    "W1, B1 = dense1.get_weights()\n",
    "W2, B2 = dense2.get_weights()\n",
    "\n",
    "print(\"X : {}\\n\".format(X.shape))\n",
    "\n",
    "print(\"W1 : {}\".format(W1.shape))\n",
    "print(\"B1 : {}\".format(B1.shape))\n",
    "print(\"A1 : {}\\n\".format(A1.shape))\n",
    "\n",
    "print(\"W2 : {}\".format(W2.shape))\n",
    "print(\"B2 : {}\".format(B2.shape))\n",
    "print(\"Y : {}\\n\".format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b9a7a",
   "metadata": {},
   "source": [
    "## Code.2-2-2: Dense Layers with python List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2e20c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (4, 10)\n",
      "After dense layer 0\n",
      "(4, 10)\n",
      "After dense layer 1\n",
      "(4, 20)\n",
      "After dense layer 2\n",
      "(4, 30)\n",
      "After dense layer 3\n",
      "(4, 40)\n",
      "After dense layer 4\n",
      "(4, 50)\n",
      "After dense layer 5\n",
      "(4, 60)\n",
      "After dense layer 6\n",
      "(4, 70)\n",
      "After dense layer 7\n",
      "(4, 80)\n",
      "After dense layer 8\n",
      "(4, 90)\n",
      "After dense layer 9\n",
      "(4, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 진행 순서\\n1. 입력 X와 n_neurons값을 설정하여 정의한다.\\n2. dense_layers라는 list를 만든다.\\n3. n_neuron 리스트에 있는 값들을 활용해서 Dense매소드로 연산을 하는 dense를 통채로 dense_layers 리스트에 삽입한다.\\n4. 삽입된 각 layer의 연산들을 이제 입력값인 X를 넣어 연산해준다.\\n5. 연산되어 나온 값은 그 다음의 dense로 연산된다.\\n6. 그렇게 연속으로 쭉 for문을 활용하여 진행한다.\\n7. 진행되는 과정에서 사이에 print(dense_idx)와 print(X.shape)를 확인해본다.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_features =4, 10\n",
    "X = tf.random.normal(shape=(N,n_feature))\n",
    "print(\"input:\", X.shape)\n",
    "\n",
    "n_neurons = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "dense_layers = list()\n",
    "for n_neuron in n_neurons:\n",
    "    dense = Dense(units = n_neuron, activation = \"relu\")\n",
    "    dense_layers.append(dense)\n",
    "#dense_layer에는 dense자체가 들어가있다. print(dense_layers)\n",
    "\n",
    "for dense_idx, dense in enumerate(dense_layers):\n",
    "#enumerate는 for문에서 활용하는데, (list)안에 있는 것을 (idex,원소)로 형성하여 tuble형태로 만들어준다.\n",
    "    X = dense(X) #matrix형태의 X자체를 dense에 넣어 바로 연산을 진행한다.\n",
    "    print(\"After dense layer\", dense_idx)\n",
    "    print(X.shape) #첫번째 layer에 들어갔다가 연산된 X를 출력한다.\n",
    "Y = X\n",
    "\n",
    "\"\"\" 진행 순서\n",
    "1. 입력 X와 n_neurons값을 설정하여 정의한다.\n",
    "2. dense_layers라는 list를 만든다.\n",
    "3. n_neuron 리스트에 있는 값들을 활용해서 Dense매소드로 연산을 하는 dense를 통채로 dense_layers 리스트에 삽입한다.\n",
    "4. 삽입된 각 layer의 연산들을 이제 입력값인 X를 넣어 연산해준다.\n",
    "5. 연산되어 나온 값은 그 다음의 dense로 연산된다.\n",
    "6. 그렇게 연속으로 쭉 for문을 활용하여 진행한다.\n",
    "7. 진행되는 과정에서 사이에 print(dense_idx)와 print(X.shape)를 확인해본다.\n",
    "\"\"\"\n",
    "#확인하여 보면, 처음 입력된 X의 N값은 계속해서 행의 수로 지속되고, layer에 들어갔다 나오면, layer의 neuron개수로 열의 수가 변하는 것을 확인할 수 있다.\n",
    "#이유는, X(4x10) * W(10xneuron개수) -> X(4xneuron개수) 행렬의 곱을 생각해보면 이해할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd8bbd",
   "metadata": {},
   "source": [
    "## Code.2-2-3: Output Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22a6e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(Tensorflow): \n",
      " [[0.25245297 0.46066585 0.4525585  0.45102328 0.5903147 ]\n",
      " [0.23759148 0.4002252  0.40926906 0.45166448 0.59273547]\n",
      " [0.24389961 0.46314526 0.44465926 0.44369826 0.5874325 ]\n",
      " [0.26002067 0.44658163 0.4542893  0.4591606  0.59370977]]\n",
      "T(Menual): \n",
      " [[0.25245297 0.46066585 0.4525585  0.45102328 0.5903147 ]\n",
      " [0.23759148 0.4002252  0.40926906 0.45166448 0.59273547]\n",
      " [0.24389961 0.46314526 0.44465926 0.44369826 0.5874325 ]\n",
      " [0.26002067 0.44658163 0.4542893  0.4591606  0.59370977]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.linalg import matmul\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "N, n_features =4, 10 #4개의 데이터 셈플\n",
    "X = tf.random.normal(shape=(N,n_feature))\n",
    "X_cp = tf.identity(X) \n",
    "#X_cp=X로 해버리면 컴퓨터는 X_cp와 X의 주소값이 같다고 생각하여, 동일하게 계속 변동이 될것이므로, 따로 주소값을 두기위해 tf.identity로 뒀다.\n",
    "\n",
    "n_neurons = [3,4,5]\n",
    "\n",
    "dense_layers = list()\n",
    "\n",
    "for n_neuron in n_neurons:\n",
    "    dense = Dense(units = n_neuron, activation = \"sigmoid\")\n",
    "    dense_layers.append(dense)\n",
    "#dense_layer에는 dense자체가 들어가있다. print(dense_layers)\n",
    "\n",
    "#forward propagation(Tensorflow)\n",
    "W, B = list(), list()\n",
    "for dense_idx, dense in enumerate(dense_layers):\n",
    "    X = dense(X) \n",
    "    w,b = dense.get_weights()\n",
    "    #Weight와 Bias값도 list를 만들어 각 리스트에 삽입시키자.\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "print(\"Y(Tensorflow): \\n\", X.numpy())\n",
    "         \n",
    "#forward propagation(Menual) menual 계산을 위해 위에 w와 b의 list를 만든것이다.\n",
    "for layer_idx in range(len(n_neurons)):\n",
    "    w,b  =W[layer_idx], B[layer_idx] #위에서 만든 W, B 리스트에서 neurons의 순서에 맞게 꺼내어 affine function 과 activation function을 진행한다.\n",
    "    \n",
    "    X_cp = matmul(X_cp,w)+b\n",
    "    X_cp = 1/(1+exp(-X_cp))\n",
    "print(\"T(Menual): \\n\", X_cp.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a1cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
